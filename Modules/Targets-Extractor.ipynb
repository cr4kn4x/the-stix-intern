{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.teleprompt import MIPROv2\n",
    "import utils\n",
    "import os\n",
    "import typing\n",
    "import json\n",
    "\n",
    "from BasicHtmlToTextParser import BasicHtmlToTextParser\n",
    "from metrics import stixnet_f1, semantic_match_hungarian\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_core import ValidationError\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 100 total: 147 train-ratio: 0.3197278911564626 dev-ratio: 0.6802721088435374\n"
     ]
    }
   ],
   "source": [
    "dataset = utils.get_dspy_examples_randomized(\"../LADDER-Dataset/\", BasicHtmlToTextParser(include_images=False), random_seed=1337)\n",
    "\n",
    "trainset, devset = utils.split_dataset(split_at=47, dataset=dataset)\n",
    "\n",
    "trainset, devset = utils.generate_targets_dataset(trainset), utils.generate_targets_dataset(devset)\n",
    "\n",
    "trainset_ids, devset_ids = [example.id for example in trainset], [example.id for example in devset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_3_1_8b_instruct_deepinfra = dspy.LM(model=\"openai/meta-llama/Meta-Llama-3.1-8B-Instruct\", api_key=os.environ.get(\"DEEPINFRA_API_KEY\"), base_url=\"https://api.deepinfra.com/v1/openai\", temperature=0.1, max_tokens=1024)\n",
    "\n",
    "llama_3_1_8b_instruct_no_cache_deepinfra = dspy.LM(model=\"openai/meta-llama/Meta-Llama-3.1-8B-Instruct\", api_key=os.environ.get(\"DEEPINFRA_API_KEY\"), base_url=\"https://api.deepinfra.com/v1/openai\", temperature=0.3, max_tokens=1024, cache=False, cache_in_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llama_3_1_70b_instruct_turbo_deepinfra = dspy.LM(model=\"openai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\", api_key=os.environ.get(\"DEEPINFRA_API_KEY\"), base_url=\"https://api.deepinfra.com/v1/openai\", temperature=0.1, max_tokens=1024)\n",
    "\n",
    "llama_3_1_70b_instruct_turbo_no_cache_deepinfra = dspy.LM(model=\"openai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\", api_key=os.environ.get(\"DEEPINFRA_API_KEY\"), base_url=\"https://api.deepinfra.com/v1/openai\", temperature=0.3, max_tokens=1024, cache=False, cache_in_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "llama_3_2_3b_instruct_deepinfra = dspy.LM(model=\"openai/meta-llama/Llama-3.2-3B-Instruct\", api_key=os.environ.get(\"DEEPINFRA_API_KEY\"), base_url=\"https://api.deepinfra.com/v1/openai\", temperature=0.1, max_tokens=1024)\n",
    "\n",
    "llama_3_2_3b_instruct_no_cache_deepinfra = dspy.LM(model=\"openai/meta-llama/Llama-3.2-3B-Instruct\", api_key=os.environ.get(\"DEEPINFRA_API_KEY\"), base_url=\"https://api.deepinfra.com/v1/openai\", temperature=0.3, max_tokens=1024, cache=False, cache_in_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "llama_3_2_1b_instruct_deepinfra = dspy.LM(model=\"openai/meta-llama/Llama-3.2-1B-Instruct\", api_key=os.environ.get(\"DEEPINFRA_API_KEY\"), base_url=\"https://api.deepinfra.com/v1/openai\", temperature=0.1, max_tokens=1024)\n",
    "\n",
    "llama_3_2_1b_instruct_no_cache_deepinfra = dspy.LM(model=\"openai/meta-llama/Llama-3.2-1B-Instruct\", api_key=os.environ.get(\"DEEPINFRA_API_KEY\"), base_url=\"https://api.deepinfra.com/v1/openai\", temperature=0.3, max_tokens=1024, cache=False, cache_in_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# qwen \n",
    "qwen_2_5_7b_instruct_deepinfra = dspy.LM(model=\"openai/Qwen/Qwen2.5-7B-Instruct\", api_key=os.environ.get(\"DEEPINFRA_API_KEY\"), base_url=\"https://api.deepinfra.com/v1/openai\", temperature=0.1, max_tokens=1024)\n",
    "\n",
    "qwen_2_5_7b_instruct_no_cache_deepinfra = dspy.LM(model=\"openai/Qwen/Qwen2.5-7B-Instruct\", api_key=os.environ.get(\"DEEPINFRA_API_KEY\"), base_url=\"https://api.deepinfra.com/v1/openai\", temperature=0.3, max_tokens=1024, cache=False, cache_in_memory=False)\n",
    "\n",
    "\n",
    "qwen_2_5_72b_instruct_deepinfra = dspy.LM(model=\"openai/Qwen/Qwen2.5-72B-Instruct\", api_key=os.environ.get(\"DEEPINFRA_API_KEY\"), base_url=\"https://api.deepinfra.com/v1/openai\", temperature=0.1, max_tokens=1024)\n",
    "\n",
    "qwen_2_5_72b_instruct_no_cache_deepinfra = dspy.LM(model=\"openai/Qwen/Qwen2.5-72B-Instruct\", api_key=os.environ.get(\"DEEPINFRA_API_KEY\"), base_url=\"https://api.deepinfra.com/v1/openai\", temperature=0.3, max_tokens=1024, cache=False, cache_in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", token=os.environ.get(\"HF_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9383]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "sentences = [\"Adobe Flash-Player\", \"Adobe Flash-Player Version 2.4\"]\n",
    "\n",
    "embedding_1 = model.encode(sentences[0], convert_to_tensor=True)\n",
    "embedding_2 = model.encode(sentences[1], convert_to_tensor=True)\n",
    "\n",
    "util.pytorch_cos_sim(embedding_1, embedding_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECISION_STORE, RECALL_STORE, F1_STORE = [], [], []\n",
    "\n",
    "def metric(example, pred, trace=None): \n",
    "    \n",
    "    malwares_with_target = set()\n",
    "    for triple in example.targets_triples: \n",
    "        malwares_with_target.add(triple.source)\n",
    "\n",
    "    gold_malwares = [malware.lower() for malware in malwares_with_target]\n",
    "    pred_malwares = list({triple.source.lower() for triple in pred.targets_triples})\n",
    "    malware_matches = list(set(gold_malwares) & set(pred_malwares))\n",
    "\n",
    "\n",
    "\n",
    "    gold_target_locations_by_malware = defaultdict(set)\n",
    "    gold_target_software_by_malware = defaultdict(set)\n",
    "    for triple in example.targets_triples:\n",
    "        source_malware = triple.source.lower()\n",
    "        assert triple.source_type == \"malware\"\n",
    "\n",
    "        if triple.target_type == \"location\":\n",
    "            gold_target_locations_by_malware[source_malware].add(triple.target)\n",
    "        elif triple.target_type == \"tool\":\n",
    "            gold_target_software_by_malware[source_malware].add(triple.target)\n",
    "       \n",
    "\n",
    "    pred_target_locations_by_malware = defaultdict(set)\n",
    "    pred_target_software_by_malware = defaultdict(set)\n",
    "    for triple in pred.targets_triples:\n",
    "        source_malware = triple.source.lower()\n",
    "        assert triple.source_type == \"malware\"\n",
    "\n",
    "        if triple.target_type == \"location\":\n",
    "            pred_target_locations_by_malware[source_malware].add(triple.target)\n",
    "        elif triple.target_type == \"tool\":\n",
    "            pred_target_software_by_malware[source_malware].add(triple.target)\n",
    "    \n",
    "    total_tp, total_fp, total_fn = 0, 0, 0 \n",
    "    \n",
    "    for malware in malware_matches: \n",
    "        gold_locations = gold_target_locations_by_malware.get(malware, set())\n",
    "        pred_locations = pred_target_locations_by_malware.get(malware, set())\n",
    "\n",
    "        tp_loc, fp_loc, fn_loc = semantic_match_hungarian(pred_locations, gold_locations, model=model, threshold=0.9)\n",
    "\n",
    "        gold_software = gold_target_software_by_malware.get(malware, set())\n",
    "        pred_software = pred_target_software_by_malware.get(malware, set())\n",
    "\n",
    "        tp_soft, fp_soft, fn_soft = semantic_match_hungarian(pred_software, gold_software, model=model, threshold=0.9)\n",
    "\n",
    "        total_tp += tp_loc + tp_soft\n",
    "        total_fp += fp_loc + fp_soft\n",
    "        total_fn += fn_loc + fn_soft\n",
    "\n",
    "\n",
    "    missing_malwares = set(gold_malwares) - set(pred_malwares)\n",
    "    for missing_malware in missing_malwares:\n",
    "        total_fn += len(gold_target_locations_by_malware.get(missing_malware, []))\n",
    "        total_fn += len(gold_target_software_by_malware.get(missing_malware, []))\n",
    "\n",
    "    extra_malwares = set(pred_malwares) - set(gold_malwares)\n",
    "    for extra_malware in extra_malwares:\n",
    "        total_fp += len(pred_target_locations_by_malware.get(extra_malware, []))\n",
    "        total_fp += len(pred_target_software_by_malware.get(extra_malware, []))\n",
    "\n",
    "\n",
    "    precision, recall, f1 = stixnet_f1(total_tp, total_fp, total_fn)\n",
    "\n",
    "    PRECISION_STORE.append(precision)\n",
    "    RECALL_STORE.append(recall)\n",
    "    F1_STORE.append(f1)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def save_and_evaluate(program: dspy.Predict, llm: dspy.LM, llm_no_cache: dspy.LM, llm_id: str, base_path: str, valset: typing.List[dspy.Example]):\n",
    "    # #################################################################################################\n",
    "    global PRECISION_STORE\n",
    "    global RECALL_STORE\n",
    "    global F1_STORE\n",
    "\n",
    "    PRECISION_STORE, RECALL_STORE, F1_STORE = [], [], []\n",
    "    # #################################################################################################\n",
    "    \n",
    "    retry_stats = []\n",
    "    for i, obj in enumerate(valset): \n",
    "        \n",
    "        print(f\"{i+1}/{len(valset)}\")\n",
    "\n",
    "        retries, max_retries = 0, 5\n",
    "        while True:\n",
    "            try: \n",
    "                print(f\"Retry {retries}/{max_retries}\")\n",
    "                if retries > 0:\n",
    "                    with dspy.settings.context(lm=llm_no_cache):\n",
    "                        targets_triples = program(**obj.inputs()).targets_triples\n",
    "                else:\n",
    "                    with dspy.settings.context(lm=llm):\n",
    "                        targets_triples = program(**obj.inputs()).targets_triples\n",
    "\n",
    "                \n",
    "                pred = dspy.Prediction(targets_triples=targets_triples)\n",
    "\n",
    "                f1 = metric(obj, pred)\n",
    "                retry_stats.append({\"finished\": True, \"retries\": retries})\n",
    "                print(f\"✔️ done with {retries} retries\")\n",
    "                break\n",
    "\n",
    "            except ValidationError as e:\n",
    "                retries += 1\n",
    "                if retries == max_retries:  \n",
    "                    retry_stats.append({\"finished\": False, \"retries\": retries})\n",
    "                    RECALL_STORE.append(0)\n",
    "                    PRECISION_STORE.append(0)\n",
    "                    F1_STORE.append(0)\n",
    "                    print(f\"❌ Failed after {retries} retries\")\n",
    "                    break\n",
    "                \n",
    "            except Exception as e: \n",
    "                retries += 1\n",
    "                if retries == max_retries:  \n",
    "                    retry_stats.append({\"finished\": False, \"retries\": retries})\n",
    "                    RECALL_STORE.append(0)\n",
    "                    PRECISION_STORE.append(0)\n",
    "                    F1_STORE.append(0)\n",
    "                    print(f\"❌ Failed after {retries} retries\")\n",
    "                    break    \n",
    "            \n",
    "    # store result\n",
    "    with open(f\"{base_path}/{llm_id}_precision.json\", \"w\") as fp:\n",
    "        json.dump(PRECISION_STORE, fp)\n",
    "\n",
    "    with open(f\"{base_path}/{llm_id}_recall.json\", \"w\") as fp:\n",
    "        json.dump(RECALL_STORE, fp)\n",
    "\n",
    "    with open(f\"{base_path}/{llm_id}_f1.json\", \"w\") as fp:\n",
    "        json.dump(F1_STORE, fp)\n",
    "\n",
    "    with open(f\"{base_path}/{llm_id}_retry_stats.json\", \"w\") as fp:\n",
    "        json.dump(retry_stats, fp)\n",
    "\n",
    "    return retry_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in devset + trainset:\n",
    "    assert metric(example, example) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class triple(BaseModel):\n",
    "    source: str = Field(description=\"name of the STIX Domain Object (SDO)\")\n",
    "    source_type: typing.Literal[\"attack_pattern\", \"campaign\", \"intrusion_set\", \"malware\", \"threat_actor\"] = Field(description=\"type of the SDO\")\n",
    "\n",
    "    relationship: typing.Literal[\"targets\"] = Field(description=\"STIX Relationship Object (SRO) that connects the source object with the target\")\n",
    "\n",
    "    target: str = Field(description=\"name of the STIX Domain Object (SDO) that is the target. Written exactly as in the threat_report (like a directly cited quote)\")\n",
    "    target_type: typing.Literal[\"identity\", \"location\", \"vulnerability\", \"infrastructure\", \"tool\"] = Field(description=\"type of the SDO\")\n",
    "\n",
    "\n",
    "\n",
    "def enforce_stix(attack_pattern_triples: typing.List[triple]):\n",
    "    filered_triples = []\n",
    "\n",
    "    for triple in attack_pattern_triples:\n",
    "        if triple.source_type == \"malware\":\n",
    "            if (triple.relationship == \"targets\") and (triple.target_type == \"tool\" or triple.target_type == \"location\"):\n",
    "                filered_triples.append(triple)\n",
    "    \n",
    "    return filered_triples\n",
    "\n",
    "\n",
    "class TargetsExtractionSignature(dspy.Signature):\n",
    "    \"\"\"\n",
    "    You are a cyber threat intelligence expert and your task is to analyse the threat report and extract all meaningful targets being targeted by entities mentioned in the threat report. \n",
    "\n",
    "    The following triples are considered as valid as they conform to the STIX 2.1 Specification. Only extract triples that match these rules: \n",
    "        <attack_pattern, targets, identity>\n",
    "        <attack_pattern, targets, location>\n",
    "        <attack_pattern, targets, vulnerability>\n",
    "        <attack_pattern, targets, tool>\n",
    "\n",
    "        <campaign, targets, identity>\n",
    "        <campaign, targets, location>\n",
    "        <campaign, targets, vulnerability>\n",
    "\n",
    "        <intrusion_set, targets, vulnerability>\n",
    "        <intrusion_set, targets, identity>\n",
    "        <intrusion_set, targets, location>\n",
    "\n",
    "        <malware, targets, identity>\n",
    "        <malware, targets, infrastructure>\n",
    "        <malware, targets, vulnerability>\n",
    "        <malware, targets, location>\n",
    "\n",
    "        <threat_actor, targets, identity>\n",
    "        <threat_actor, targets, location>\n",
    "        <threat_actor, targets, vulnerability>\n",
    "\n",
    "    *Final hints*\n",
    "        - Make sure to stricly cite the target word by word\n",
    "        - Consider mentioned_malwares, mentioned_threat_actors as hints for the source of the triples\n",
    "    \"\"\"\n",
    "    threat_report: str = dspy.InputField()\n",
    "    mentioned_malwares: typing.List[str] = dspy.InputField(desc=\"A list of malware names that are mentioned in the threat report\")\n",
    "    mentioned_threat_actors: typing.List[str] = dspy.InputField(desc=\"A list of threat actors that are mentioned in the threat report\")\n",
    "    targets_triples: typing.List[triple] = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"ALL DONE\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/Baseline\"\n",
    "mlflow.autolog()\n",
    "mlflow.set_experiment(BASE_PATH)\n",
    "\n",
    "to_do = [\n",
    "    (\"llama_3_2_1b_instruct\", llama_3_2_1b_instruct_deepinfra, llama_3_2_1b_instruct_no_cache_deepinfra), # done \n",
    "    (\"llama_3_2_3b_instruct\", llama_3_2_3b_instruct_deepinfra, llama_3_2_3b_instruct_no_cache_deepinfra), # done\n",
    "    (\"llama_3_1_8b_instruct\", llama_3_1_8b_instruct_deepinfra, llama_3_1_8b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_70b_instruct\", llama_3_1_70b_instruct_turbo_deepinfra, llama_3_1_70b_instruct_turbo_no_cache_deepinfra),\n",
    "\n",
    "    (\"qwen_2_5_7b_instruct\", qwen_2_5_7b_instruct_deepinfra, qwen_2_5_7b_instruct_no_cache_deepinfra),\n",
    "    (\"qwen_2_5_72b_instruct\", qwen_2_5_72b_instruct_deepinfra, qwen_2_5_72b_instruct_no_cache_deepinfra),\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do:\n",
    "    program = dspy.Predict(TargetsExtractionSignature)\n",
    "    \n",
    "    save_and_evaluate(program=program, llm=llm, llm_no_cache=llm_no_cache, llm_id=llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use for example vllm  https://github.com/vllm-project/vllm\n",
    "API_KEY = None\n",
    "BASE_URL = None\n",
    "qwen_2_5_1p5b_instruct_vllm = dspy.LM(\"openai/Qwen/Qwen2.5-1.5B-Instruct\",  api_key=API_KEY, base_url=BASE_URL, temperature=0.1, max_tokens=1024)\n",
    "qwen_2_5_1p5b_instruct_no_cache_vllm = dspy.LM(\"openai/Qwen/Qwen2.5-1.5B-Instruct\",  api_key=API_KEY, base_url=BASE_URL, temperature=0.3, max_tokens=1024, cache=False, cache_in_memory=False)\n",
    "\n",
    "\n",
    "qwen_2_5_3b_instruct_vllm = dspy.LM(model=\"openai/Qwen/Qwen2.5-3B-Instruct\", api_key=API_KEY, base_url=BASE_URL, temperature=0.1, max_tokens=1024)\n",
    "qwen_2_5_3b_instruct_no_cache_vllm = dspy.LM(model=\"openai/Qwen/Qwen2.5-3B-Instruct\", api_key=API_KEY, base_url=BASE_URL, temperature=0.3, max_tokens=1024, cache=False, cache_in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"DONE\"\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_1p5b_instruct\", qwen_2_5_1p5b_instruct_vllm, qwen_2_5_1p5b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/Baseline\"\n",
    "\n",
    "mlflow.autolog()\n",
    "mlflow.set_experiment(BASE_PATH)\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do:\n",
    "    program = dspy.Predict(TargetsExtractionSignature)\n",
    "    \n",
    "    save_and_evaluate(program=program, llm=llm, llm_no_cache=llm_no_cache, llm_id=llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"DONE\"\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_3b_instruct\", qwen_2_5_3b_instruct_vllm, qwen_2_5_3b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/Baseline\"\n",
    "\n",
    "mlflow.autolog()\n",
    "mlflow.set_experiment(BASE_PATH)\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    program = dspy.Predict(TargetsExtractionSignature)\n",
    "    save_and_evaluate(program=program, llm=llm, llm_no_cache=llm_no_cache, llm_id=llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZERO SHOT OPTIMIERUNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1_optimizer_settings = dict(num_candidates = 10, max_bootstrapped_demos = 0, max_labeled_demos = 0, metric_threshold = None, init_temperature = 0.5, task_model = None, num_threads = 16, max_errors = 30, prompt_model=llama_3_1_70b_instruct_turbo_deepinfra, teacher_settings=dict(lm=llama_3_1_70b_instruct_turbo_deepinfra), auto=None)\n",
    "\n",
    "\n",
    "o1_compiler_settings = dict(num_trials = 30, minibatch = True, minibatch_size = 25, minibatch_full_eval_steps = 10, program_aware_proposer=True, data_aware_proposer=True, view_data_batch_size=10, tip_aware_proposer=True, fewshot_aware_proposer=True, requires_permission_to_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"already finished and optimizer steps are not cached!\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/ZERO-O1\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"llama_3_2_1b_instruct\", llama_3_2_1b_instruct_deepinfra, llama_3_2_1b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_2_3b_instruct\", llama_3_2_3b_instruct_deepinfra, llama_3_2_3b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_8b_instruct\", llama_3_1_8b_instruct_deepinfra, llama_3_1_8b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_70b_instruct\", llama_3_1_70b_instruct_turbo_deepinfra, llama_3_1_70b_instruct_turbo_no_cache_deepinfra),\n",
    "\n",
    "    (\"qwen_2_5_7b_instruct\", qwen_2_5_7b_instruct_deepinfra, qwen_2_5_7b_instruct_no_cache_deepinfra),\n",
    "    (\"qwen_2_5_72b_instruct\", qwen_2_5_72b_instruct_deepinfra, qwen_2_5_72b_instruct_no_cache_deepinfra), # tbd\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    default_program = dspy.Predict(TargetsExtractionSignature)\n",
    "\n",
    "    o1_optimizer = MIPROv2(metric=metric, **o1_optimizer_settings)\n",
    "                    \n",
    "    with dspy.settings.context(lm=llm):\n",
    "        optimized_program = o1_optimizer.compile(student=default_program, trainset=trainset, valset=devset, **o1_compiler_settings)\n",
    "\n",
    "    # store program (cloudpickle)\n",
    "    optimized_program.save(f\"{BASE_PATH}/{llm_id}\", save_program=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"already finished and optimizer steps are not cached!\"\n",
    "BASE_PATH = \"./TargetsExtractor/ZERO-O1\"\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_1p5b_instruct\", qwen_2_5_1p5b_instruct_vllm, qwen_2_5_1p5b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    default_program = dspy.Predict(TargetsExtractionSignature)\n",
    "    \n",
    "    o1_optimizer = MIPROv2(metric=metric, **o1_optimizer_settings)\n",
    "                    \n",
    "    with dspy.settings.context(lm=llm):\n",
    "        optimized_program = o1_optimizer.compile(student=default_program, trainset=trainset, valset=devset, **o1_compiler_settings)\n",
    "\n",
    "    # store program (cloudpickle)\n",
    "    optimized_program.save(f\"{BASE_PATH}/{llm_id}\", save_program=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"already finished and optimizer steps are not cached!\"\n",
    "BASE_PATH = \"./TargetsExtractor/ZERO-O1\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_3b_instruct\", qwen_2_5_3b_instruct_vllm, qwen_2_5_3b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    default_program = dspy.Predict(TargetsExtractionSignature)\n",
    "    \n",
    "    o1_optimizer = MIPROv2(metric=metric, **o1_optimizer_settings)\n",
    "                    \n",
    "    with dspy.settings.context(lm=llm):\n",
    "        optimized_program = o1_optimizer.compile(student=default_program, trainset=trainset, valset=devset, **o1_compiler_settings)\n",
    "\n",
    "    # store program (cloudpickle)\n",
    "    optimized_program.save(f\"{BASE_PATH}/{llm_id}\", save_program=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2_optimizer_settings = dict(num_candidates = 25, max_bootstrapped_demos = 0, max_labeled_demos = 0, metric_threshold = None, init_temperature = 0.5, task_model = None, num_threads = 24,max_errors = 30, prompt_model=llama_3_1_70b_instruct_turbo_deepinfra, teacher_settings=dict(lm=llama_3_1_70b_instruct_turbo_deepinfra), auto=None)\n",
    "\n",
    "\n",
    "o2_compiler_settings = dict(num_trials = 50, minibatch = True, minibatch_size = 25, minibatch_full_eval_steps = 10, program_aware_proposer=True, data_aware_proposer=True, view_data_batch_size=10, tip_aware_proposer=True, fewshot_aware_proposer=True, requires_permission_to_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"already finished and optimizer steps are not cached!\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/ZERO-O2\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"llama_3_2_1b_instruct\", llama_3_2_1b_instruct_deepinfra, llama_3_2_1b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_2_3b_instruct\", llama_3_2_3b_instruct_deepinfra, llama_3_2_3b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_8b_instruct\", llama_3_1_8b_instruct_deepinfra, llama_3_1_8b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_70b_instruct\", llama_3_1_70b_instruct_turbo_deepinfra, llama_3_1_70b_instruct_turbo_no_cache_deepinfra),\n",
    "\n",
    "    (\"qwen_2_5_7b_instruct\", qwen_2_5_7b_instruct_deepinfra, qwen_2_5_7b_instruct_no_cache_deepinfra),\n",
    "    (\"qwen_2_5_72b_instruct\", qwen_2_5_72b_instruct_deepinfra, qwen_2_5_72b_instruct_no_cache_deepinfra),\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    default_program = dspy.Predict(TargetsExtractionSignature)\n",
    "\n",
    "    o2_optimizer = MIPROv2(metric=metric, **o2_optimizer_settings)\n",
    "                    \n",
    "    with dspy.settings.context(lm=llm):\n",
    "        optimized_program = o2_optimizer.compile(student=default_program, trainset=trainset, valset=devset, **o2_compiler_settings)\n",
    "\n",
    "    # store program (cloudpickle)\n",
    "    optimized_program.save(f\"{BASE_PATH}/{llm_id}\", save_program=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"already finished and optimizer steps are not cached!\"\n",
    "BASE_PATH = \"./TargetsExtractor/ZERO-O2\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_1p5b_instruct\", qwen_2_5_1p5b_instruct_vllm, qwen_2_5_1p5b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    default_program = dspy.Predict(TargetsExtractionSignature)\n",
    "    \n",
    "    o2_optimizer = MIPROv2(metric=metric, **o2_optimizer_settings)\n",
    "                    \n",
    "    with dspy.settings.context(lm=llm):\n",
    "        optimized_program = o2_optimizer.compile(student=default_program, trainset=trainset, valset=devset, **o2_compiler_settings)\n",
    "\n",
    "    # store program (cloudpickle)\n",
    "    optimized_program.save(f\"{BASE_PATH}/{llm_id}\", save_program=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"already finished and optimizer steps are not cached!\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/ZERO-O2\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_3b_instruct\", qwen_2_5_3b_instruct_vllm, qwen_2_5_3b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    default_program = dspy.Predict(TargetsExtractionSignature)\n",
    "    \n",
    "    o2_optimizer = MIPROv2(metric=metric, **o2_optimizer_settings)\n",
    "                    \n",
    "    with dspy.settings.context(lm=llm):\n",
    "        optimized_program = o2_optimizer.compile(student=default_program, trainset=trainset, valset=devset, **o2_compiler_settings)\n",
    "\n",
    "    # store program (cloudpickle)\n",
    "    optimized_program.save(f\"{BASE_PATH}/{llm_id}\", save_program=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEW SHOT OPTIMIERUNG O1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1_optimizer_settings = dict(num_candidates = 10, max_bootstrapped_demos = 1, max_labeled_demos = 2, metric_threshold = None, init_temperature = 0.5, task_model = None, num_threads = 64, max_errors = 100, prompt_model=llama_3_1_70b_instruct_turbo_deepinfra, teacher_settings=dict(lm=llama_3_1_70b_instruct_turbo_deepinfra), auto=None)\n",
    "\n",
    "\n",
    "o1_compiler_settings = dict(num_trials = 30, minibatch = True, minibatch_size = 25, minibatch_full_eval_steps = 10, program_aware_proposer=True, data_aware_proposer=True, view_data_batch_size=10, tip_aware_proposer=True, fewshot_aware_proposer=True, requires_permission_to_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"already finished and optimizer steps are not cached!\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/FS-O1\"\n",
    "\n",
    "to_do = [\n",
    "    (\"llama_3_2_1b_instruct\", llama_3_2_1b_instruct_deepinfra, llama_3_2_1b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_2_3b_instruct\", llama_3_2_3b_instruct_deepinfra, llama_3_2_3b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_8b_instruct\", llama_3_1_8b_instruct_deepinfra, llama_3_1_8b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_70b_instruct\", llama_3_1_70b_instruct_turbo_deepinfra, llama_3_1_70b_instruct_turbo_no_cache_deepinfra),\n",
    "\n",
    "    (\"qwen_2_5_7b_instruct\", qwen_2_5_7b_instruct_deepinfra, qwen_2_5_7b_instruct_no_cache_deepinfra),\n",
    "    (\"qwen_2_5_72b_instruct\", qwen_2_5_72b_instruct_deepinfra, qwen_2_5_72b_instruct_no_cache_deepinfra),\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    default_program = dspy.Predict(TargetsExtractionSignature)\n",
    "\n",
    "    o1_optimizer = MIPROv2(metric=metric, **o1_optimizer_settings)\n",
    "                    \n",
    "    with dspy.settings.context(lm=llm):\n",
    "        optimized_program = o1_optimizer.compile(student=default_program, trainset=trainset, valset=devset, **o1_compiler_settings)\n",
    "\n",
    "    # store program (cloudpickle)\n",
    "    optimized_program.save(f\"{BASE_PATH}/{llm_id}\", save_program=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"already finished and optimizer steps are not cached!\"\n",
    "BASE_PATH = \"./TargetsExtractor/FS-O1\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_1p5b_instruct\", qwen_2_5_1p5b_instruct_vllm, qwen_2_5_1p5b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    default_program = dspy.Predict(TargetsExtractionSignature)\n",
    "    \n",
    "    o1_optimizer = MIPROv2(metric=metric, **o1_optimizer_settings)\n",
    "                    \n",
    "    with dspy.settings.context(lm=llm):\n",
    "        optimized_program = o1_optimizer.compile(student=default_program, trainset=trainset, valset=devset, **o1_compiler_settings)\n",
    "\n",
    "    # store program (cloudpickle)\n",
    "    optimized_program.save(f\"{BASE_PATH}/{llm_id}\", save_program=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"already finished and optimizer steps are not cached!\"\n",
    "BASE_PATH = \"./TargetsExtractor/FS-O1\"\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_3b_instruct\", qwen_2_5_3b_instruct_vllm, qwen_2_5_3b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    default_program = dspy.Predict(TargetsExtractionSignature)\n",
    "    \n",
    "    o1_optimizer = MIPROv2(metric=metric, **o1_optimizer_settings)\n",
    "                    \n",
    "    with dspy.settings.context(lm=llm):\n",
    "        optimized_program = o1_optimizer.compile(student=default_program, trainset=trainset, valset=devset, **o1_compiler_settings)\n",
    "\n",
    "    # store program (cloudpickle)\n",
    "    optimized_program.save(f\"{BASE_PATH}/{llm_id}\", save_program=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEW SHOT OPTIMIERUNG O2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2_optimizer_settings = dict(num_candidates = 25, max_bootstrapped_demos = 1, max_labeled_demos = 2, metric_threshold = None, init_temperature = 0.5, task_model = None, num_threads = 24, max_errors = 100, prompt_model=llama_3_1_70b_instruct_turbo_deepinfra, teacher_settings=dict(lm=llama_3_1_70b_instruct_turbo_deepinfra), auto=None)\n",
    "\n",
    "\n",
    "o2_compiler_settings = dict(num_trials = 50, minibatch = True, minibatch_size = 25, minibatch_full_eval_steps = 10, program_aware_proposer=True, data_aware_proposer=True, view_data_batch_size=10, tip_aware_proposer=True, fewshot_aware_proposer=True, requires_permission_to_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"already finished and optimizer steps are not cached!\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/FS-O2\"\n",
    "\n",
    "to_do = [\n",
    "    (\"llama_3_2_1b_instruct\", llama_3_2_1b_instruct_deepinfra, llama_3_2_1b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_2_3b_instruct\", llama_3_2_3b_instruct_deepinfra, llama_3_2_3b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_8b_instruct\", llama_3_1_8b_instruct_deepinfra, llama_3_1_8b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_70b_instruct\", llama_3_1_70b_instruct_turbo_deepinfra, llama_3_1_70b_instruct_turbo_no_cache_deepinfra),\n",
    "\n",
    "    (\"qwen_2_5_7b_instruct\", qwen_2_5_7b_instruct_deepinfra, qwen_2_5_7b_instruct_no_cache_deepinfra),\n",
    "    (\"qwen_2_5_72b_instruct\", qwen_2_5_72b_instruct_deepinfra, qwen_2_5_72b_instruct_no_cache_deepinfra),\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    default_program = dspy.Predict(TargetsExtractionSignature)\n",
    "\n",
    "    o2_optimizer = MIPROv2(metric=metric, **o2_optimizer_settings)\n",
    "                    \n",
    "    with dspy.settings.context(lm=llm):\n",
    "        optimized_program = o2_optimizer.compile(student=default_program, trainset=trainset, valset=devset, **o2_compiler_settings)\n",
    "\n",
    "    # store program (cloudpickle)\n",
    "    optimized_program.save(f\"{BASE_PATH}/{llm_id}\", save_program=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"already finished and optimizer steps are not cached!\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/FS-O2\"\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_1p5b_instruct\", qwen_2_5_1p5b_instruct_vllm, qwen_2_5_1p5b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    default_program = dspy.Predict(TargetsExtractionSignature)\n",
    "    \n",
    "    o2_optimizer = MIPROv2(metric=metric, **o2_optimizer_settings)\n",
    "                    \n",
    "    with dspy.settings.context(lm=llm):\n",
    "        optimized_program = o2_optimizer.compile(student=default_program, trainset=trainset, valset=devset, **o2_compiler_settings)\n",
    "\n",
    "    # store program (cloudpickle)\n",
    "    optimized_program.save(f\"{BASE_PATH}/{llm_id}\", save_program=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"already finished and optimizer steps are not cached!\"\n",
    "BASE_PATH = \"./TargetsExtractor/FS-O2\"\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_3b_instruct\", qwen_2_5_3b_instruct_vllm, qwen_2_5_3b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    default_program = dspy.Predict(TargetsExtractionSignature)\n",
    "    \n",
    "    o2_optimizer = MIPROv2(metric=metric, **o2_optimizer_settings)\n",
    "                    \n",
    "    with dspy.settings.context(lm=llm):\n",
    "        optimized_program = o2_optimizer.compile(student=default_program, trainset=trainset, valset=devset, **o2_compiler_settings)\n",
    "\n",
    "    # store program (cloudpickle)\n",
    "    optimized_program.save(f\"{BASE_PATH}/{llm_id}\", save_program=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION OF THE OPTIMIZED PROGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"DONE\"\n",
    "BASE_PATH = \"./TargetsExtractor/ZERO-O1\"\n",
    "\n",
    "to_do = [\n",
    "    (\"llama_3_2_1b_instruct\", llama_3_2_1b_instruct_deepinfra, llama_3_2_1b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_2_3b_instruct\", llama_3_2_3b_instruct_deepinfra, llama_3_2_3b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_8b_instruct\", llama_3_1_8b_instruct_deepinfra, llama_3_1_8b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_70b_instruct\", llama_3_1_70b_instruct_turbo_deepinfra, llama_3_1_70b_instruct_turbo_no_cache_deepinfra),\n",
    "\n",
    "    (\"qwen_2_5_7b_instruct\", qwen_2_5_7b_instruct_deepinfra, qwen_2_5_7b_instruct_no_cache_deepinfra),\n",
    "    (\"qwen_2_5_72b_instruct\", qwen_2_5_72b_instruct_deepinfra, qwen_2_5_72b_instruct_no_cache_deepinfra),\n",
    "]\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    program = dspy.load(f\"{BASE_PATH}/{llm_id}\")\n",
    "    save_and_evaluate(program, llm, llm_no_cache, llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"DONE\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/ZERO-O1\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_1p5b_instruct\", qwen_2_5_1p5b_instruct_vllm, qwen_2_5_1p5b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    program = dspy.load(f\"{BASE_PATH}/{llm_id}\")\n",
    "\n",
    "    save_and_evaluate(program, llm, llm_no_cache, llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"DONE\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/ZERO-O1\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_3b_instruct\", qwen_2_5_3b_instruct_vllm, qwen_2_5_3b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    program = dspy.load(f\"{BASE_PATH}/{llm_id}\")\n",
    "\n",
    "    save_and_evaluate(program, llm, llm_no_cache, llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"DONE\"\n",
    "BASE_PATH = \"./TargetsExtractor/ZERO-O2\"\n",
    "\n",
    "to_do = [\n",
    "    (\"llama_3_2_1b_instruct\", llama_3_2_1b_instruct_deepinfra, llama_3_2_1b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_2_3b_instruct\", llama_3_2_3b_instruct_deepinfra, llama_3_2_3b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_8b_instruct\", llama_3_1_8b_instruct_deepinfra, llama_3_1_8b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_70b_instruct\", llama_3_1_70b_instruct_turbo_deepinfra, llama_3_1_70b_instruct_turbo_no_cache_deepinfra),\n",
    "\n",
    "    (\"qwen_2_5_7b_instruct\", qwen_2_5_7b_instruct_deepinfra, qwen_2_5_7b_instruct_no_cache_deepinfra),\n",
    "    (\"qwen_2_5_72b_instruct\", qwen_2_5_72b_instruct_deepinfra, qwen_2_5_72b_instruct_no_cache_deepinfra),\n",
    "]\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    program = dspy.load(f\"{BASE_PATH}/{llm_id}\")\n",
    "\n",
    "    save_and_evaluate(program, llm, llm_no_cache, llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"DONE\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/ZERO-O2\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_1p5b_instruct\", qwen_2_5_1p5b_instruct_vllm, qwen_2_5_1p5b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    program = dspy.load(f\"{BASE_PATH}/{llm_id}\")\n",
    "\n",
    "    save_and_evaluate(program, llm, llm_no_cache, llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"DONE\"\n",
    "BASE_PATH = \"./TargetsExtractor/ZERO-O2\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_3b_instruct\", qwen_2_5_3b_instruct_vllm, qwen_2_5_3b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    program = dspy.load(f\"{BASE_PATH}/{llm_id}\")\n",
    "\n",
    "    save_and_evaluate(program, llm, llm_no_cache, llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEW SHOT Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"DONE\"\n",
    "BASE_PATH = \"./TargetsExtractor/FS-O1\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"llama_3_2_1b_instruct\", llama_3_2_1b_instruct_deepinfra, llama_3_2_1b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_2_3b_instruct\", llama_3_2_3b_instruct_deepinfra, llama_3_2_3b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_8b_instruct\", llama_3_1_8b_instruct_deepinfra, llama_3_1_8b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_70b_instruct\", llama_3_1_70b_instruct_turbo_deepinfra, llama_3_1_70b_instruct_turbo_no_cache_deepinfra),\n",
    "\n",
    "    (\"qwen_2_5_7b_instruct\", qwen_2_5_7b_instruct_deepinfra, qwen_2_5_7b_instruct_no_cache_deepinfra),\n",
    "    (\"qwen_2_5_72b_instruct\", qwen_2_5_72b_instruct_deepinfra, qwen_2_5_72b_instruct_no_cache_deepinfra),\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    program = dspy.load(f\"{BASE_PATH}/{llm_id}\")\n",
    "    save_and_evaluate(program, llm, llm_no_cache, llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"DONE\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/FS-O1\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_1p5b_instruct\", qwen_2_5_1p5b_instruct_vllm, qwen_2_5_1p5b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    program = dspy.load(f\"{BASE_PATH}/{llm_id}\")\n",
    "\n",
    "    save_and_evaluate(program, llm, llm_no_cache, llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"DONE\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/FS-O1\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_3b_instruct\", qwen_2_5_3b_instruct_vllm, qwen_2_5_3b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    program = dspy.load(f\"{BASE_PATH}/{llm_id}\")\n",
    "\n",
    "    save_and_evaluate(program, llm, llm_no_cache, llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"DONE\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/FS-O2\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"llama_3_2_1b_instruct\", llama_3_2_1b_instruct_deepinfra, llama_3_2_1b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_2_3b_instruct\", llama_3_2_3b_instruct_deepinfra, llama_3_2_3b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_8b_instruct\", llama_3_1_8b_instruct_deepinfra, llama_3_1_8b_instruct_no_cache_deepinfra),\n",
    "    (\"llama_3_1_70b_instruct\", llama_3_1_70b_instruct_turbo_deepinfra, llama_3_1_70b_instruct_turbo_no_cache_deepinfra),\n",
    "\n",
    "    (\"qwen_2_5_7b_instruct\", qwen_2_5_7b_instruct_deepinfra, qwen_2_5_7b_instruct_no_cache_deepinfra),\n",
    "    (\"qwen_2_5_72b_instruct\", qwen_2_5_72b_instruct_deepinfra, qwen_2_5_72b_instruct_no_cache_deepinfra),\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    program = dspy.load(f\"{BASE_PATH}/{llm_id}\")\n",
    "\n",
    "    save_and_evaluate(program, llm, llm_no_cache, llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"DONE\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/FS-O2\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_1p5b_instruct\", qwen_2_5_1p5b_instruct_vllm, qwen_2_5_1p5b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    program = dspy.load(f\"{BASE_PATH}/{llm_id}\")\n",
    "\n",
    "    save_and_evaluate(program, llm, llm_no_cache, llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"DONE\"\n",
    "\n",
    "BASE_PATH = \"./TargetsExtractor/FS-O2\"\n",
    "\n",
    "\n",
    "to_do = [\n",
    "    (\"qwen_2_5_3b_instruct\", qwen_2_5_3b_instruct_vllm, qwen_2_5_3b_instruct_no_cache_vllm)\n",
    "]\n",
    "\n",
    "\n",
    "for llm_id, llm, llm_no_cache in to_do: \n",
    "    program = dspy.load(f\"{BASE_PATH}/{llm_id}\")\n",
    "    save_and_evaluate(program, llm, llm_no_cache, llm_id, base_path=BASE_PATH, valset=devset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
